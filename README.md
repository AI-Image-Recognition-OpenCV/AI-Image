# AI OpenCV ì˜ìƒ ì²˜ë¦¬ ğŸ§‘ğŸ‘©
ì´ í”„ë¡œì íŠ¸ëŠ” ì›¹ìº ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì–¼êµ´ì„ ê°ì§€í•˜ê³ , ê°ì§€ëœ ì–¼êµ´ ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ì—¬ ì¼ì • ê±°ë¦¬ ì´ë‚´ë¡œ ì ‘ê·¼í–ˆì„ ë•Œ ê²½ê³ ìŒì„ ì¬ìƒí•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.

## ê¸°ëŠ¥
+ OpenCVë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì§€.
+ ê°ì§€ëœ ì–¼êµ´ ê°„ì˜ ê±°ë¦¬ ì¸¡ì •.
+ ì–¼êµ´ ê°„ ê±°ë¦¬ê°€ ì¼ì • ì„ê³„ê°’ ì´í•˜ì¼ ë•Œ ê²½ê³ ìŒ ì¬ìƒ.
## ìš”êµ¬ ì‚¬í•­
+ Python 3.x
+ OpenCV
+ NumPy
+ gTTS
+ pygame
+ Yolov3
    * yolov3.weights : YOLOv3 ëª¨ë¸ì˜ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨.
    * yolov3.cfg : YOLOv3 ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜.
```
import cv2
import numpy as np
import pyttsx3

#YOLO ê°ì²´ ê°ì§€ í•¨ìˆ˜
def yoloNet(image, net, output_layers, CONF_VALUE=0.5):
    (h, w) = image.shape[:2]
    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    persons_detected = False

    class_ids = []
    confidences = []
    boxes = []

    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > CONF_VALUE and class_id == 0:  # COCO ë°ì´í„°ì…‹ì—ì„œ ì‚¬ëŒì˜ class_idëŠ” 0ì…ë‹ˆë‹¤
                persons_detected = True
                center_x = int(detection[0] * w)
                center_y = int(detection[1] * h)
                width = int(detection[2] * w)
                height = int(detection[3] * h)
                x = int(center_x - width / 2)
                y = int(center_y - height / 2)
                boxes.append([x, y, width, height])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, CONF_VALUE, 0.4)

    for i in range(len(boxes)):
        if i in indexes:
            x, y, w, h = boxes[i]
            label = f"Person: {confidences[i]:.2f}"
            color = (0, 255, 0)  # ì‚¬ëŒì˜ ê²½ìš° ì´ˆë¡ìƒ‰ ì‚¬ê°í˜•
            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
            cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    if persons_detected:
        texttospeech("ë…¸ë€ì„  ì•ˆìœ¼ë¡œ ë“¤ì–´ê°€ì„¸ìš”.")

    return image

#í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def texttospeech(text):
    # pyttsx3 ì—”ì§„ ì´ˆê¸°í™”
    engine = pyttsx3.init()

    # ì†ì„± ì¡°ì • (ì†ë„, ë³¼ë¥¨, ëª©ì†Œë¦¬)
    rate = engine.getProperty('rate')
    engine.setProperty('rate', rate - 50)
    volume = engine.getProperty('volume')
    engine.setProperty('volume', volume + 0.25)

    # í•œêµ­ì–´ ëª©ì†Œë¦¬ ì„¤ì • (ì‹œìŠ¤í…œì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    voices = engine.getProperty('voices')
    for voice in voices:
        if 'ko' in voice.id:  # í•œêµ­ì–´ ëª©ì†Œë¦¬ ì°¾ê¸°
            engine.setProperty('voice', voice.id)
            break

    # í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
    engine.say(text)
    engine.runAndWait()

#YOLO ëª¨ë¸ ë¡œë“œ ë° íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
yolo_weights = 'yolov3.weights'
yolo_cfg = 'yolov3.cfg'
net = cv2.dnn.readNetFromDarknet(yolo_cfg, yolo_weights)
layer_names = net.getLayerNames()
output_layers_indexes = net.getUnconnectedOutLayers()

#ì¶œë ¥ ë ˆì´ì–´ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
output_layers = [layer_names[i - 1] for i in output_layers_indexes]

#ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ë‹¤ë¥¸ ë¹„ë””ì˜¤ íŒŒì¼ì´ì–´ì•¼ í•¨)
filename1 = 'images(DL)/2.mp4'
filename2 = 'images(DL)/2.mp4'

#ë‘ ê°œì˜ ë¹„ë””ì˜¤ ìº¡ì²˜ ì—´ê¸°
cap1 = cv2.VideoCapture(filename1)
cap2 = cv2.VideoCapture(filename2)

if not cap1.isOpened() or not cap2.isOpened():
    print("Error: Could not open video.")
    exit()

#í”„ë ˆì„ ìŠ¤í‚µ ê°’
frame_skip = 2
frame_count = 0
last_combined_frame = None

#í”„ë ˆì„ ë£¨í”„
while True:
    ret1, frame1 = cap1.read()
    ret2, frame2 = cap2.read()
    if not ret1 or not ret2:
        break

    frame_count += 1
    if frame_count % frame_skip == 0:
        # ì²˜ë¦¬ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ í”„ë ˆì„ í¬ê¸° ì¡°ì •
        small_frame1 = cv2.resize(frame1, (640, 480))
        small_frame2 = cv2.resize(frame2, (640, 480))

        # ë‘ ë²ˆì§¸ í”„ë ˆì„ì— YOLO ê°ì²´ ê°ì§€ ìˆ˜í–‰
        detected_frame = yoloNet(small_frame2, net, output_layers)

        # ì›ë³¸ê³¼ ê°ì§€ëœ í”„ë ˆì„ì„ ìˆ˜ì§ìœ¼ë¡œ ê²°í•©
        combined_frame = np.vstack((small_frame1, detected_frame))

        # ê²°í•©ëœ í”„ë ˆì„ í‘œì‹œ
        cv2.imshow('Original (top) and Object Detection (bottom)', combined_frame)
        last_combined_frame = combined_frame  # ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬ëœ í”„ë ˆì„ ì €ì¥

    # ìŠ¤í˜ì´ìŠ¤ë°”ê°€ ëˆŒë¦¬ë©´ ì¢…ë£Œ
    if cv2.waitKey(1) & 0xFF == ord(' '):
        break

#ë¹„ë””ì˜¤ê°€ ëë‚˜ë©´ ë§ˆì§€ë§‰ í”„ë ˆì„ ê³„ì† í‘œì‹œ
if last_combined_frame is not None:
    while True:
        cv2.imshow('Original (top) and Object Detection (bottom)', last_combined_frame)
        if cv2.waitKey(1) & 0xFF == ord(' '):
            break

#ë¦¬ì†ŒìŠ¤ í•´ì œ
cap1.release()
cap2.release()
cv2.destroyAllWindows()
```
## ì„¤ëª…
ì´ í”„ë¡œì íŠ¸ëŠ” YOLO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ëŒì„ ê°ì§€í•˜ê³ , ê°ì§€ëœ ì‚¬ëŒì´ ì¼ì • ê±°ë¦¬ ì´ë‚´ë¡œ ì ‘ê·¼í•˜ë©´ ê²½ê³ ìŒì„ ì¬ìƒí•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.
